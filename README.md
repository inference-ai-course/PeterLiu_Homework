ðŸ§  Machine Learning Engineering Homework

This repository contains my homework submissions for the Machine Learning Engineering (MLE) program.
Each week's submission is organized into its own folder (e.g., homework1/, homework2/, etc.), with code, documentation, and screenshots (if applicable).

ðŸ”— Project Repository

<<<<<<< HEAD
## ðŸ”— Project Repository
ðŸ‘‰ [PeterLiu_Homework on GitHub](https://github.com/inference-ai-course/PeterLiu_Homework)
=======
ðŸ‘‰ PeterLiu_Homework on GitHub

ðŸ“ Contents
Folder	Description
homework1/	LangChain-based AI agent using Ollama and Gradio
homework2/	End-to-end data pipeline: web/audio scraping, cleaning, deduplication
homework3/	Voice-based LLM assistant with ASR, local LLM, and TTS via FastAPI
homework4/	Retrieval-Augmented Generation (RAG) system with arXiv papers
homework5/	Advanced RAG: hybrid retrieval and evaluation framework
homework6/	Voice Agent with Function Calling (local deployment)
homework7/	LLM Fine-Tuning with LoRA  QLoRA and unsloth
README.md	This file (overview of all weekly submissions)
>>>>>>> 83a8edf (Save local updates before pulling)

âœ… Week 1 â€“ Build Your First AI Agent

<<<<<<< HEAD
## ðŸ“ Contents

| Folder      | Description                                                                 |
|-------------|-----------------------------------------------------------------------------|
| homework1/  | LangChain-based AI agent using Ollama and Gradio                            |
| homework2/  | End-to-end data pipeline: web/audio scraping, cleaning, deduplication       |
| homework3/  | Voice-based LLM assistant with ASR, local LLM, and TTS via FastAPI          |
| homework4/  | Retrieval-Augmented Generation (RAG) system with arXiv papers               |
| homework5/  | Advanced RAG: hybrid retrieval and evaluation framework                     |
| README.md   | This file (overview of all weekly submissions)                              |

---

## âœ… Week 1 â€“ Build Your First AI Agent
ðŸŽ¯ **Goal:**  
Set up a local LLM workflow and build a plugin-enabled AI assistant.

**What was done:**
- Installed and ran LLMs locally using Ollama  
- Integrated LangChain and Gradio to build an interactive UI  
- Built a plugin-capable agent that could call external tools:
  - Brave Search ðŸ”
  - Puppeteer for web scraping ðŸ“„
  - Filesystem tools ðŸ“
  - Notion and GitHub APIs ðŸ§ ðŸ™  
- (Optional) Automatically scraped and saved data to Notion  
=======
ðŸŽ¯ Goal:
Set up a local LLM workflow and build a plugin-enabled AI assistant.

What was done:

Installed and ran LLMs locally using Ollama
>>>>>>> 83a8edf (Save local updates before pulling)

Integrated LangChain and Gradio to build an interactive UI

<<<<<<< HEAD
## âœ… Week 2 â€“ Data Collection & Cleaning Pipeline
ðŸŽ¯ **Goal:**  
Create a reproducible pipeline for collecting and cleaning multimodal data (text, audio, image).

**What was done:**
- Performed web scraping (text & PNG) and audio crawling (.mp3)  
- Applied content extraction, language filtering, and standardization  
- Used MinHash + datasketch to detect and remove duplicates  
- Saved final clean dataset in JSONL format  
- All steps implemented as modular Jupyter Notebooks  
=======
Built a plugin-capable agent that could call external tools:

Brave Search ðŸ”

Puppeteer for web scraping ðŸ“„

Filesystem tools ðŸ“

Notion and GitHub APIs ðŸ§ ðŸ™

(Optional) Automatically scraped and saved data to Notion

âœ… Week 2 â€“ Data Collection & Cleaning Pipeline

ðŸŽ¯ Goal:
Create a reproducible pipeline for collecting and cleaning multimodal data (text, audio, image).

What was done:

Performed web scraping (text & PNG) and audio crawling (.mp3)
>>>>>>> 83a8edf (Save local updates before pulling)

Applied content extraction, language filtering, and standardization

<<<<<<< HEAD
## âœ… Week 3 â€“ Build a Local Voice-Based AI Assistant
ðŸŽ¯ **Goal:**  
Create a modular voice-based AI assistant that processes voice input and generates spoken responses using local models.

**What was done:**
- Built an ASR â†’ LLM â†’ TTS pipeline served via FastAPI  
- Used Faster-Whisper or Whisper.cpp to transcribe speech  
- Queried local LLaMA3 model via Ollama for responses  
- Converted output text to speech via Edge-TTS  
- Added multi-turn memory to maintain conversational context  
- Exposed via `/voice-chat/` API endpoint  
- (Optional) Dockerized the entire project for deployment  
=======
Used MinHash + datasketch to detect and remove duplicates

Saved final clean dataset in JSONL format

All steps implemented as modular Jupyter Notebooks

âœ… Week 3 â€“ Build a Local Voice-Based AI Assistant

ðŸŽ¯ Goal:
Create a modular voice-based AI assistant that processes voice input and generates spoken responses using local models.

What was done:

Built an ASR â†’ LLM â†’ TTS pipeline served via FastAPI
>>>>>>> 83a8edf (Save local updates before pulling)

Used Faster-Whisper or Whisper.cpp to transcribe speech

<<<<<<< HEAD
## âœ… Week 4 â€“ Retrieval-Augmented Generation (RAG) with arXiv Papers
ðŸŽ¯ **Goal:**  
Build a foundational Retrieval-Augmented Generation (RAG) system tailored to scientific research.  

**What was done:**
- Collected 50 recent arXiv cs.CL PDF papers (via scraping or sample set)  
- Extracted and cleaned text from PDFs using PyMuPDF  
- Chunked documents into â‰¤512-token overlapping segments (sliding window)  
- Generated dense vector embeddings with Sentence-Transformers (`all-MiniLM-L6-v2`)  
- Built a FAISS index (IndexFlatL2) for fast vector similarity search  
- Developed a notebook demo for querying the index and displaying top text chunks  
- Exposed a `/search` FastAPI endpoint to accept queries and return top passages in JSON  

---

## âœ… Week 5 â€“ Advanced RAG with Hybrid Retrieval
ðŸŽ¯ **Goal:**  
Extend the RAG system with hybrid retrieval (dense + sparse methods) and design an evaluation pipeline for retrieval quality.  

**What was done:**
- Integrated **BM25** retriever with FAISS dense retriever â†’ hybrid retrieval strategy  
- Implemented weighted scoring and rank fusion to combine dense + sparse results  
- Added evaluation using:
  - Recall@K, MRR (Mean Reciprocal Rank), nDCG  
  - Human-in-the-loop spot checks for relevance  
- Enhanced pipeline modularity (retriever registry, configurable backends)  
- Built an interactive notebook for side-by-side retrieval comparison  
- Extended FastAPI service with `/hybrid-search` endpoint  
=======
Queried local LLaMA3 model via Ollama for responses

Converted output text to speech via Edge-TTS

Added multi-turn memory to maintain conversational context

Exposed via /voice-chat/ API endpoint

(Optional) Dockerized the entire project for deployment

âœ… Week 4 â€“ Retrieval-Augmented Generation (RAG) with arXiv Papers
>>>>>>> 83a8edf (Save local updates before pulling)

ðŸŽ¯ Goal:
Build a foundational RAG system tailored to scientific research.

<<<<<<< HEAD
## ðŸ“¸ Screenshots
Screenshots for each homework are stored in the respective folders:
- `homework1/screenshots/`
- `homework2/screenshots/`
- `homework3/screenshots/` (if applicable)  
- `homework4/screenshots/` (run homework4 on GPU server)
- `homework5/screenshots/` (evaluation  demo)  
=======
What was done:

Collected 50 recent arXiv cs.CL PDF papers (via scraping or sample set)

Extracted and cleaned text from PDFs using PyMuPDF
>>>>>>> 83a8edf (Save local updates before pulling)

Chunked documents into â‰¤512-token overlapping segments (sliding window)

<<<<<<< HEAD
## ðŸ“ž Contact
For any questions or clarifications, feel free to reach out via Canvas or email.

=======
Generated dense vector embeddings with Sentence-Transformers (all-MiniLM-L6-v2)

Built a FAISS index (IndexFlatL2) for fast vector similarity search

Developed a notebook demo for querying the index and displaying top text chunks

Exposed a /search FastAPI endpoint to accept queries and return top passages in JSON

âœ… Week 5 â€“ Advanced RAG with Hybrid Retrieval

ðŸŽ¯ Goal:
Extend the RAG system with hybrid retrieval (dense + sparse methods) and design an evaluation pipeline for retrieval quality.

What was done:

Integrated BM25 retriever with FAISS dense retriever â†’ hybrid retrieval strategy

Implemented weighted scoring and rank fusion to combine dense + sparse results

Added evaluation using:

Recall@K, MRR (Mean Reciprocal Rank), nDCG

Human-in-the-loop spot checks for relevance

Enhanced pipeline modularity (retriever registry, configurable backends)

Built an interactive notebook for side-by-side retrieval comparison

Extended FastAPI service with /hybrid-search endpoint

âœ… Week 6 â€“ Voice Agent with Function Calling (Local Deployment)

ðŸŽ¯ Goal:
Build a fully local voice assistant that supports speech input/output and function calling.

What was done:

Implemented text + audio input via Streamlit frontend

Offline speech-to-text (STT) with faster-whisper

Added function calling support:

calculate(expression) â†’ math calculation

search_arxiv(query) â†’ simulated arXiv paper search

Integrated text-to-speech (TTS) using pyttsx3

Displayed multi-turn chat history in UI

Local deployment:

FastAPI backend (/docs for API testing)

Streamlit frontend (/voice for interaction)

Extensible design â†’ add new tools in backend.py, swap STT/TTS for streaming

âœ… Week 7 â€“ LLM Fine-Tuning (LoRA & QLoRA)

ðŸŽ¯ Goal:
Perform parameter-efficient fine-tuning of large language models using LoRA and QLoRA.

What was done:

Set up fine-tuning pipeline with Hugging Face transformers + peft

Implemented LoRA adapters for efficient training on limited GPU resources

Used QLoRA with bitsandbytes for 4-bit quantization â†’ reduced VRAM usage

Trained models on domain-specific datasets (instruction-style)

Evaluated fine-tuned models vs. base model on benchmark prompts

Saved and reloaded fine-tuned adapters for inference & deployment

ðŸ“¸ Screenshots

Screenshots for each homework are stored in the respective folders:

homework1/screenshots/

homework2/screenshots/

homework3/screenshots/

homework4/screenshots/

homework5/screenshots/(run on Inference.ai Cloud Server)

homework6/screenshots/

homework7/screenshots/ (run on Inference.ai Cloud Server)

ðŸ“ž Contact

For any questions or clarifications, feel free to reach out via Canvas or email.
>>>>>>> 83a8edf (Save local updates before pulling)


