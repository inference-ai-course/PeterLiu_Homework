===========================
Test started at 2025-08-04 22:11:36

[GET /]
{"message":"Multi-PDF RAG Search API"}-e 

[POST /search] Query: "transformer model"
Time: .293112000s
{
  "query": "transformer model",
  "results": [
    {
      "document": "9812001v3.pdf",
      "chunk": "the thesaurus tree. The number of free parameters equals the number of nodes in the cut minus one. The class of ‘linear regression models’ is also an example model class. A discrete model is a0 + a1 · x1 + · · · + ak · xk + ǫ, where xi(i = 1, · · ·, k) denotes a random variable, ai(i = 0, 1, · · ·, k) a parameter, and ǫ a random variable based on the standard normal distribution N(0, 1). The number of parameters in this model equals (k + 1). A class of models can be denoted as M = {Pθ(X) : θ ∈Θ(m), m ∈M}, where m stands for a discrete model, M a set of discrete models, θ a parameter vector, and Θ(m) a parameter space associated with m. Usually we assume that the model class we introduced contains the ‘true’ model which has given rise to the data, but it does not matter if it does not. In such case, the best model selected from the class can be considered an approximation of the true model. The model class we introduce reﬂects our prior knowledge on the problem. Total description length We next consider how to calculate total description length. Total description length equals the sum total of the code length for encoding a dis- crete model (model description length l(m)), the code length for encoding parameters given the discrete model (parameter description length l(θ|m)), and the code length 2.7. INTRODUCTION TO MDL 27 for encoding the data given the discrete model and the parameters (data description length l(xn|m, θ)). Note that we also sometimes refer to the model description length as l(m) + l(θ|m). Our goal is to ﬁnd the minimum description length of the data (in number of bits) with respect to the model class, namely, Lmin(xn : M) = min m∈M min θ∈Θ(m) \u0010 l(m) + l(θ|m) + l(xn|m, θ) \u0011 . Model description length Let us ﬁrst consider how to calculate model description length l(m). The choice of a code for encoding discrete models is subjective; it depends on our prior knowledge on the model class. If the set of discrete models M is ﬁnite and the probability distribution over it is a uniform distribution, i.e., P(m) = 1 |M|, m ∈M, then we need l(m) = log |M| to encode each discrete model m using a non-redundant code. If M is a countable set, i.e., each of its members can be assigned a positive integer, then the ‘Elias code,’ which is usually used for encoding integers, can be employed (Rissanen, 1989). Letting i be the integer assigned to a discrete model m, we need l(m) = log c + log i + log log i + · · · to encode m. Here the sum includes all the positive iterates and c denotes a constant of about 2.865. Parameter description length and data description length When a discrete model m is ﬁxed, a parameter space will be uniquely determined. The model class turns out to be Mm = {Pθ(X)"
    },
    {
      "document": "9811022v2.pdf",
      "chunk": "predictor model and parser; • estimate a separate word predictor for left-to- right language modeling. Note that the correspond- ing model predictor was obtained via re-estimation aimed at increasing the probability of the ”N-best” parses of the entire sentence; • reduce vocabulary of parser operations; extreme case: no non-terminal labels/POS tags, word only model; this will increase the speed of the parser thus rendering it usable on larger amounts of train- ing data and allowing the use of deeper stacks — resulting in more “N-best” derivations per sentence during re-estimation; • relax — ﬂatten — the initial statistics in the re- estimation of model parameters; this would allow the model parameters to converge to a diﬀerent point that might yield a lower word-level perplexity; • evaluate model performance on n-best sentences output by an automatic speech recognizer. 7 Acknowledgments This research has been funded by the NSF IRI-19618874 grant (STIMULATE). The authors would like to thank to Sanjeev Khu- danpur for his insightful suggestions. Also to Harry Printz, Eric Ristad, Andreas Stolcke, Dekai Wu and all the other members of the dependency model- ing group at the summer96 DoD Workshop for use- ful comments on the model, programming support and an extremely creative environment. Also thanks to Eric Brill, Sanjeev Khudanpur, David Yarowsky, Radu Florian, Lidia Mangu and Jun Wu for useful input during the meetings of the people working on our STIMULATE grant. References W. Byrne, A. Gunawardana, and S. Khudanpur. 1998. Information geometry and EM variants. Technical Report CLSP Research Note 17, De- partment of Electical and Computer Engineering, The Johns Hopkins University, Baltimore, MD. C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu- danpur, L. Mangu, H. Printz, E. S. Ristad, R. Rosenfeld, A. Stolcke, and D. Wu. 1997. Struc- ture and performance of a dependency language model. In Proceedings of Eurospeech, volume 5, pages 2775–2778. Rhodes, Greece. Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceed- ings of the 34th Annual Meeting of the Associ- ation for Computational Linguistics, pages 184– 191. Santa Cruz, CA. A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. In Journal of the Royal Statistical Society, volume 39 of B, pages 1–38. Frederick Jelinek and Robert Mercer. 1980. Inter- polated estimation of markov source parameters from sparse data. In E. Gelsema and L. Kanal, ed- itors, Pattern Recognition in Practice, pages 381– 397. F. Jelinek, J. Laﬀerty, D. M. Magerman, R. Mercer, A. Ratnaparkhi, and S. Roukos. 1994. Decision tree parsing using a hidden derivational model. In ARPA, editor, Proceedings of the Human Lan- guage Technology Workshop, pages 272–277. M. Marcus, B. Santorini, and M. Marcinkiewicz. 1995. Building a large annotated corpus of En- glish: the Penn Treebank. Computational Lin- guistics, 19(2):313–330. Colin Philips. 1996. Order and Structure. Ph.D. thesis, MIT. Distributed by MITWPL."
    },
    {
      "document": "9812001v3.pdf",
      "chunk": ". . . . . . . . . . 99 6.3 PP-attachment disambiguation results. . . . . . . . . . . . . . . . . . . 100 7.1 PP-attachment disambiguation results. . . . . . . . . . . . . . . . . . . 105 7.2 Results reported in previous work. . . . . . . . . . . . . . . . . . . . . . 105 8.1 Models proposed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 8.2 Algorithm employed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 List of Figures 1.1 Organization of this thesis. . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1 Frequency data for the subject slot for verb ‘ﬂy.’ . . . . . . . . . . . . . 11 2.2 Word-based distribution estimated using MLE. . . . . . . . . . . . . . . 13 2.3 Example co-occurrence data. . . . . . . . . . . . . . . . . . . . . . . . . 15 3.1 An example hard co-occurrence model. . . . . . . . . . . . . . . . . . . 44 3.2 Relations between models. . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.1 An example thesaurus. . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.2 A tree cut model with [swallow, crow, eagle, bird, bug, bee, insect]. . . 54 4.3 A tree cut model with [BIRD, bug, bee, insect]. . . . . . . . . . . . . . 55 4.4 A tree cut model with [BIRD, INSECT]. . . . . . . . . . . . . . . . . . 55 4.5 The Find-MDL algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.6 An example application of Find-MDL. . . . . . . . . . . . . . . . . . . 61 4.7 Example generalization result (for the arg2 slot for ‘eat’). . . . . . . . . 63 4.8 Accuracy-coverage plots for MDL, SA, and LA. . . . . . . . . . . . . . 67 5.1 Example dependency forests."
    },
    {
      "document": "9812001v3.pdf",
      "chunk": "and subsample ST ′, deﬁne L(ST ′|ΓT ′, ˆθT ′) to be the data description length of subsample ST ′ using submodel MT ′, deﬁne L(ˆθT ′|ΓT ′) to be the parameter description length for the submodel MT ′, and deﬁne L′(MT ′, ST ′) to be L(ST ′|ΓT ′, ˆθT ′) + L(ˆθT ′|ΓT ′). First for any (sub)tree T, for any (sub)model MT = (ΓT, ˆθT) which is contained in T except the (sub)model consisting only of the root node of T, and for any (sub)sample ST contained in T, we have L(ST|ΓT, ˆθT) = k X i=1 L(STi|ΓTi, ˆθTi), (A.3) where Ti, (i = 1, · · ·, k) denote the child subtrees of T. For any (sub)tree T, for any (sub)model MT = (ΓT, ˆθT) which is contained in T A.5. EQUIVALENT DEPENDENCY TREE MODELS 131 except the (sub)model consisting only of the root node of T, we have L(ˆθT |ΓT) = k X i=1 L(ˆθTi|ΓTi), (A.4) where Ti, (i = 1, · · ·, k) denote the child subtrees of T. When T is the entire thesaurus tree, the parameter description length for a tree cut model in T should be L(ˆθT |ΓT) = k X i=1 L(ˆθTi|ΓTi) −log |S| 2 , (A.5) where |S| is the size of the entire sample. Since the second term −log |S| 2 in (A.5) is common to each model in the entire thesaurus tree, it is irrelevant for the purpose of ﬁnding a model with the minimum description length. We will thus use identity (A.4) both when T is a proper subtree and when it is the entire tree. (This allows us to use the same recursive algorithm (Find-MDL) in all cases.) It follows from (A.3) and (A.4) that the minimization of description length can be done essentially independently for each subtree. Namely, if we let L′ min(MT, ST) denote the minimum description length (as deﬁned by (A.3) and (A.4)) achievable for (sub)model MT on (sub)sample ST contained in (sub)tree T, ˆP(η) the MLE estimate of the probability for node η, and root(T) the root node of T, then we have L′ min(MT, ST) = min{ Pk i=1 L′ min(MTi, STi), L′(([root(T)], [ ˆP(root(T))]), ST)}. (A.6) Here, Ti, (i = 1, · · · , k) denote the child subtrees of T. The rest of the proof proceeds by induction. First, if T is a subtree having a single node, then there is only one submodel in T, and it is clearly the submodel with the minimum description length. Next, inductively assume that Find-MDL(T ′) correctly outputs a submodel with the minimum description length for any subtree T ′ of size less than n. Then, given a (sub)tree T of size n whose root node has at least two child subtrees, say Ti : i = 1, · · ·, k, for each Ti, Find-MDL(Ti) returns a submodel with the minimum description length by inductive hypothesis. Then, since (A.6) holds, in whichever way the if-clause on lines 8, 9 of Find-MDL is evaluated, what is returned on line 11 or"
    },
    {
      "document": "9812001v3.pdf",
      "chunk": "on human intuition. 5.3.4 Experiment 4: simulation In order to test how large a data size is required to estimate a dependency forest model, I conducted the following experiment. I deﬁned an artiﬁcial model in the form of a dependency forest model and generated data on the basis of its distribution. I then used the obtained data to estimate a model, and evaluated the estimated model by measuring the KL divergence between the estimated model and the true model. I also checked the number of dependency links in the obtained model. I repeatedly generated data and observed the ‘learning curve,’ namely the relationship between the data size used in estimation and the number of links in the estimated model, and the relationship between the data size and the KL divergence separating the estimated and the true model. I deﬁned two other artiﬁcial models and conducted the same experiments. Figures 5.5 and 5.6 show the results of these experiments for the three artiﬁcial models averaged over 10 trials. The number of parameters in Model 1, Model 2, and Model 3 are 18, 30, and 44 respectively, and the number of links in them 1, 3, and 5. Note that the KL divergences between the estimated models and the true models converge to 0, as expected. Also note that the numbers of links in the estimated models converge to the correct value (1, 3, and 5) in each of the three examples. These simulation results verify the consistency property of MDL (i.e., the numbers of parameters in the selected models converge in probability to that of the true model as the data size increases), which is crucial for the goal of learning dependencies. Thus we can be conﬁdent that the dependencies between case slots can be accurately learned when there are enough data, as long as the ‘true’ model exists as a dependency forest model. We also see that to estimate a model accurately the data size required is as large as 5 to 10 times the number of parameters. For example, for the KL divergence to go to below 0.1, we need more than 200 examples, which is roughly 5 to 10 times the number of parameters. Note that in Experiment 3, I considered 12 slots, and for each slot there were roughly 10 classes as its values; thus a class-based model tended to have about 120 parameters. The corpus data available to us was insuﬃcient for accurate learning of the dependencies between case slots for most verbs (cf., Table 5.3). 5.4. SUMMARY 83 5.4 Summary I conclude this chapter with the following remarks. 1. The primary contribution of the research reported in this chapter is the proposed method of learning dependencies between case slots, which is theoretically sound and eﬃcient. 2. For slot-based models, some case slots are found to be dependent. Experimental results demonstrate that by using the knowledge of dependency, when depen- dency does exist, we can signiﬁcantly improve pp-attachment disambiguation results. 3. For class-based models, most case slots are judged independent with the data size currently available in the"
    }
  ]
}
-e 

[POST /search] Query: "attention mechanism"
Time: .027993000s
{
  "query": "attention mechanism",
  "results": [
    {
      "document": "9901005v1.pdf",
      "chunk": "the critics, to assess the eﬀects they have on performance. 8.1 Evaluation of the Focus Model The algorithm presented here does not include a mechanism for recognizing the global structure of the discourse, such as in the work of Grosz and Sidner (1986), Mann and Thompson (1988), Allen and Perrault (1980), and in descendent work. Recently in the literature, Walker (1996) argues for a more linear-recency based model of attentional state (though not that discourse structure need not be recognized), while Ros´e et al. (1995) argue for a more complex model of attentional state than is represented in most current computational theories of discourse. Many theories that address how attentional state should be modeled have the goal of performing intention recognition as well. We investigate performing temporal reference resolution directly, without also attempting to recognize discourse structure or intentions. We assess the challenges the data present to our model when only this task is attempted. The total number of Temporal Units and the number of them speciﬁed by anaphoric noun phrases in the two training data sets are given in Figure 7.4 There are diﬀerent units that could be counted, from the number of temporal noun phrases to the number of distinct times referred to in the dialog. Here, we count the entities that must be resolved by a temporal reference resolution algorithm, i.e., the number of distinct temporal units speciﬁed in each sentence, summed over all sentences. Operationally, this is a count of Temporal Units after the normalization phase, i.e., after Step 1 in Section 5.2. This is the unit considered in the remainder of this paper. 4. The anaphoric counts include the cases in which both deictic and anaphoric interpretations yield the correct result. 269 Wiebe, O’Hara, ¨Ohrstr¨om-Sandgren, & McKeever To support the evaluation presented in this section, antecedent information was man- ually annotated in the training data. For each Temporal Unit speciﬁed by an anaphoric noun phrase, all of the antecedents that yield the correct interpretation under one of the anaphoric relations were identiﬁed, except that, if both TUi and TUj are appropriate an- tecedents, and one is an antecedent of the other, only the more recent one is included. Thus, only the heads of the anaphoric chains existing at that point in the dialog are included. In addition, competitor discourse entities were also identiﬁed, i.e., previously mentioned Tem- poral Units for which some relation could be established, but the resulting interpretation would be incorrect. Again, only Temporal Units at the head of an anaphoric chain were considered. To illustrate these annotations, Figure 8 shows a graph depicting anaphoric chain annotations of an NMSU dialog (dialog 9). In the ﬁgure, solid lines link the correct antecedents, dotted lines show competitors, and edges to nowhere indicate deictics. 8.1.1 Cases in which the immediately preceding time is not an appropriate antecedent The main purpose of a focus model is to make an appropriate set of discourse entities available as candidate antecedents at each point in the discourse. As described above in Section 4.3, Grosz and Sidner’s model captures situations in which"
    },
    {
      "document": "9901005v1.pdf",
      "chunk": "a ﬁeld F, where F is at least as speciﬁc as time of day, but 255 Wiebe, O’Hara, ¨Ohrstr¨om-Sandgren, & McKeever are consistent in all ﬁelds less speciﬁc than F. The resolvent contains the information in TUprevious that is less speciﬁc than F together with the information in TUcurrent that is of the same or greater speciﬁcity as F. (See rule A-modify in Section 5.3.) For example (see also (3)-(5) of the corpus example in Figure 1): Utterance Interpretation Monday looks good. Assume: Monday 19 August How about 2? (co-reference relation) 2pm, Monday 19 August Hmm, how about 4? (modify relation) 4pm, Monday 19 August 4.3 Focus Models The focus model, or model of attentional state (Grosz & Sidner, 1986), is a model of which entities the dialog is most centrally about at each point in the dialog. It determines which previously mentioned entities are the candidate antecedents of anaphoric references. As such, it represents the role that the structure of the discourse plays in reference resolution. We consider three models of attentional state in this paper: (1) the linear-recency model (see, for example, the work by Hobbs (1978) and Walker2 (1996)), (2) Grosz and Sidner’s (1986) stack-based model, and (3) the graph structured stack model introduced by Ros´e, Di Eugenio, Levin, and Van Ess-Dykema (1995). Ordered from (1) to (3), the models are successively more complex, accounting for increasingly more complex structures in the discourse. In a linear-recency based model, entities mentioned in the discourse are stored on a focus list, ordered by recency. The corresponding structure in the dialog is shown in Figure 4a: a simple progression of references, uninterrupted by subdialogs. In Grosz and Sidner’s stack-based model, the entities in focus in a particular discourse segment are stored together in a focus space associated with that segment. To handle anaphoric references across discourse segments, focus spaces are pushed on and popped oﬀ the stack as appropriate to mirror the structure of the discourse. As each new segment is recognized, a focus space is created and pushed onto the stack. To interpret an anaphoric reference, the entities in the focus space on the top of the stack are considered ﬁrst. However, if the current utterance resumes a previous discourse segment, the intervening focus spaces are popped oﬀ. This allows anaphoric reference to an earlier entity, even if more recently mentioned entities are possible antecedents (for more details, see Grosz & Sidner, 1986). Figure 4b illustrates a discourse structure that the stack-based model is designed to handle. Suppose that both TU1 and TU2 are possible antecedents of TU3 (for example, suppose they are speciﬁed by pronouns that agree in number and gender), but TU2 is in a subsegment and is not a correct antecedent of TU3, even though it is mentioned more recently than TU1. In the stack-based model, the focus space containing TU2 is popped oﬀthe stack when the end of its segment is recognized, thus removing TU2 as a competitor for understanding TU3. Following is an example from the NMSU corpus (this is the dialog segment labeled 09-09, in"
    },
    {
      "document": "9904018v1.pdf",
      "chunk": "arXiv:cs/9904018v1 [cs.CL] 24 Apr 1999 A COMPUTATIONAL MEMORY AND PROCESSING MODEL FOR PROSODY Janet E. Cahn Massachusetts Institute of Technology cahn@media.mit.edu ABSTRACT This paper links prosody to the information in the text and how it is processed by the speaker. It describes the operation and output of Loq, a text-to-speech imple- mentation that includes a model of limited attention and working memory. Attentional limitations are key. Vary- ing the attentional parameter in the simulations varies in turn what counts as given and new in a text, and there- fore, the intonational contours with which it is uttered. Currently, the system produces prosody in three diﬀer- ent styles: child-like, adult expressive, and knowledgeable. This prosody also exhibits diﬀerences within each style – no two simulations are alike. The limited resource ap- proach captures some of the stylistic and individual variety found in natural prosody. 1. INTRODUCTION Ask any lay person to imitate computer speech and you will be treated to an utterance delivered in melodic and rhythmic monotone, possibly accompanied by choppy ar- ticulation and a voice quality that is nasal and strained. In fact, current synthesized speech is far superior. Yet few would argue that synthetic and natural speech are indistin- guishable. The diﬀerence, as popular impression suggests, is the relative lack of interesting and natural variability in the synthetic version. It may be traced in part to the lack of a common causal account of pitch, timing, articulation and voice quality. Intonation and stress are usually linked to the linguistic and information structure of text. Fea- tures such as pause location and word duration are linked mainly to the speaker’s cognitive and expressive capacities, and pitch range, intensity, voice quality and articulation to her physiological and aﬀective state. In this paper, I describe a production model that at- tributes pitch and timing to the essential operations of a speaker’s working memory – the storage and retrieval of in- formation. Simulations with this model produce synthetic speech in three of the prosodic styles likely to be associ- ated with attentional and memory diﬀerences: a child-like exaggerated prosody for limited recall; a more adult but still expressive style for mid-range capacities; and a knowl- edgeable style for maximum recall. The same model also produces individual diﬀerences within each style, owing to its stochastic storage algorithm. The ability to produce both individual and genre variations supports its eventual use in prosthetic, entertainment and information applica- tions, especially in the production of reading materials for the blind and the use of computer-based autonomous and communicative agents. 2. A MEMORY MODEL FOR PROSODY Prosody organizes spoken text into phrases, and high- lights its most salient components with pitch accents, dis- tinctive pitch contours applied to the word. Pitch accents are both attentional and propositional. Their very use indicates salience; their particular form conveys a propo- sition about the words they mark. For example, speakers typically use a high pitch accent (denoted as H*) to mark salient information that they believe to be new to the ad- dressee. Conversely, when they believe the addressee is already aware"
    },
    {
      "document": "9907021v1.pdf",
      "chunk": "tries to solve focus recognition by global description of the utterance contour, in a ﬁrst approach rep- resented by the fundamental frequency F0. A reference line is computed by detecting signiﬁcant minima and maxima in the F0 contour. The average values between the maximum and minimum lines yield the global reference line. Focus accents occur mainly in the areas of steepest fall in the F0 course. Therefore, in the reference line the points with the highest negative gradient were determined ﬁrst in each utterance. To determine the position of the focus the nearest maximum in this region has been used as approximation. The recognition rate is 78.5% and the average recognition rate is 66.6%. The focus detection mod- ule sends focus hypotheses to the semantic module and to the module for transfer and generation. In a recent approach, phrase boundaries from the detector described above where integrated in the algorithm. After optimization of the algorithm even higher rates are expected. As mentioned in the last section, one of the main beneﬁts of prosody in the INTARC system is the use of prosodic phrase boundaries inside the word lattice search. When calculating a prosody factor for an edge pair, we pick the WBH associated with the connect- ing vertex of the edges. This WBH forms a sequence of WBHs and word hypotheses if combined with the portions already spanned by the pair of edges. Tests for the contribution of the prosody factor to the overall search lead to the following results: The same recognition performance in terms of n best trees could be achieved using 20% less edges on the average. A lot of edges are constant in a given search space — namely those used for the representation of the original set of word hypotheses and the empty active rule edges which have a zero span. Counting only those edges which are built up dynamically by the search process a reduction of 65% was measured. 6 In INTARC, the transfer module performs a dialog act based translation. In a traditional deep analysis it gets its input (dialog act and feature structure) from the semantic evaluation mod- ule. In an additional path a ﬂat transfer is performed with the best word chain from the word recognition module and with focus information. During shallow processing the focus accents are aligned to words. If a focus is on a content word a probabilistically selected dialog act is chosen. This dialog act is then expanded to a translation enriched with possible information from the word chain. Flat transfer is only used when deep analysis fails. First results show that the ‘focus-driven’ transfer produces correct — but sometimes reduced — results for about 50% of the data. For the other half of the utterances information is not sufﬁcient to get a translation; only 5% of the translations are absolutely wrong.. While the deep analysis uses prosody to reduce search space and disambiguate in cases of mul- tiple analyses, the ‘shallow focus based translation’ can be viewed as directly driven by prosody. 5.2 Speaker Style A new issue"
    },
    {
      "document": "9903008v1.pdf",
      "chunk": "task success and dialogue quality were also the most important performance predictors (Litman et al., 1998; Walker et al., 1998; Kamm et al., 1998). Our ﬁndings draw into question a frequently made assumption in the ﬁeld regarding the centrality of efﬁciency to per- formance, and like other recent work, demonstrates that there are important tradeoffs between efﬁciency and other performance dimensions (Danieli and Gerbino, 1995; Walker et al., 1997a). 6 Linear regression assumes that predictors are not highly correlated (e.g., because correlations above .70 can affect the coefﬁcients, deletion of redundant predictors is advised (Monge and Cappella, 1980)). There is only 1 positive correlation among our predictors (between Mean Recognition and Task Success), and it is well below .70. 5 Related Work In the area of spoken dialogue, van Zanten (1998) has proposed a method for adapting initiative in form-ﬁlling dialogues. Whenever the system rejects a user’s utterance, the system takes more initiative; whenever the user gives an over-informative answer, the system yields some initia- tive. While this method has the potential of being automated, the method has been neither fully implemented nor empirically evaluated. Smith (1998) has evaluated strategies for dynamically deciding whether to conﬁrm each user utterance during a task-oriented dialogue. Simulation re- sults suggest that context-dependent adaptation strategies can improve performance, especially when the system has greater initiative. Walker et al. (1998) and Levin and Pieraccini (1997) have used reinforcement learning to adapt dialogue behavior over time such that system performance improves. We have instead focused on optimizing performance during a single dialogue. The empirical evaluation of an adaptive interface in a commercial software system (Strachan et al., 1997) is also similar to our work. Analysis of variance demonstrated that an adaptive interface based on minimal user modeling improved subjective user satisfaction ratings. 6 Conclusion We have presented an empirical evaluation of adaptability in TOOT, a spoken dialogue system that retrieves train schedules from the web. Our results suggest that adaptable TOOT generally outperforms non-adaptable TOOT for novice users, and that the utility of adaptation is greater for UserNo TOOT than for SystemExplicit TOOT. By using analysis of variance to examine how a set of evaluation measures differ as a function of adaptability, we elaborate the conditions under which adaptability leads to greater performance. When users interact with adaptable rather than non-adaptable TOOT, User Satisfaction and Task Success are signiﬁcantly higher. These results are independent of TOOT’s initial dialogue strategy and task scenario. In contrast, Mean Recog- nition, User Expertise, and Future Use illustrate an interaction between initial dialogue strategy and adaptability. For SystemExplicit TOOT, the adaptable version does not outperform the non- adaptable version, or does not outperform the non-adaptable version very strongly. For UserNo TOOT, the adaptable version outperforms the non-adaptable version on all three measures. By using PARADISE to derive a performance function from data, we show that Mean Recog- nition, Task Success, and Elapsed Time best predict a user’s overall satisfaction with TOOT. These results help explain why adaptability in TOOT leads to overall greater performance, and allow us to make predictions about future performance."
    }
  ]
}
-e 

[POST /search] Query: "LLMs in NLP"
Time: .029822000s
{
  "query": "LLMs in NLP",
  "results": [
    {
      "document": "9812018v1.pdf",
      "chunk": "ﬂexibility by omitting unnecessary or known informa- tion from both the schemes and its IR expressions, and by including particles to increase coherency. The reports could be generated in multiple languages. We recommend the opportunistic use of shal- low techniques for this type of application. Our approach is not suitable for tasks involving deliberate sentence planning, the careful choice of lexemes, or a sophisticated distribution of information onto linguistic units. Such tasks would not be compatible with the loose coupling of our components via IR. In addition, they would require complex tests to be formulated in TGL rules, rendering the grammar rather obscure. Finally, if the intended coverage of content is to be kept extensible or is not known precisely enough at an early phase of development, the eventual redesign of the intermediate structure and associated mapping rules for text organization may severely limit the usefulness of our approach. 5 Conclusion We have suggested shallow approaches to NL generation that are suited for small applications re- quiring limited linguistic resources. While these approaches ignore many theoretical insights gained through years of NLG research and instead revive old techniques once criticized for their lack of ﬂexibility, they nevertheless allow for the quick development of running systems. By integrating techniques of diﬀerent granularity into one formalism, we have shown that lack of ﬂexibility is not an inherent property of shallow approaches. Within the air quality report generation in TEMSIS, a non-trivial application was described. We also gave a qualitative evaluation of the domain char- acteristics to be met for our approach to work successfully. Further experience will show whether shallow techniques transpose to more complex tasks. We consider it a scientiﬁc challenge to combine shallow and in-depth approaches to analysis and generation in such a way that more theoretically motivated research ﬁnds its way into real applications. References [Bateman, 1997] John Bateman. KPML delvelopment environment: multilingual linguistic resource devel- opment and sentence generation. Report, German National Center for Information Technology (GMD), Institute for integrated publication and information systems (IPSI), Darmstadt, Germany, January 1997. Release 1.1. [Busemann, 1996] Stephan Busemann. Best-ﬁrst surface realization. In Donia Scott, editor, Eighth Inter- national Natural Language Generation Workshop. Proceedings, pages 101–110, Herstmonceux, Univ. of Brighton, England, 1996. Also available at the Computation and Language Archive at cmp-lg/9605010. [Cawsey et al., 1995] Alison Cawsey, Kim Binsted, and Ray Jones. Personalised explanations for patient education. In Fifth European Workshop on Natural Language Generation. Proceedings, pages 59–74, Leiden, The Netherlands, 1995. [Davis and King, 1977] Randall Davis and Jonathan King. An overview of production systems. In E. W. Elcock and D. Michie, editors, Machine Intelligence 8, pages 300–332. Ellis Horwood, Chichester, 1977. [Elhadad and Robin, 1996] Michael Elhadad and Jacques Robin. An overview of SURGE: a reusable com- prehensive syntactic realization component. In Donia Scott, editor, Eighth International Natural Language Generation Workshop. Demonstrations and Posters, pages 1–4, Herstmonceux, Univ. of Brighton, Eng- land, 1996. [Horacek, 1996] Helmut Horacek. Lexical choice in expressing metonymic relations in multiple languages. Machine Translation, 11:109–158, 1996. [Kasper and Whitney, 1989] Robert Kasper and Richard Whitney. SPL: A"
    },
    {
      "document": "9906014v1.pdf",
      "chunk": "of estimating subtree probabilities. 1As mentioned before, a dramatic reduction has been obtained by a heuristic search algorithm. 2This result is (just) statistically signiﬁcant. We performed a paired T-test on the number of wrong semantic units per graph. This results in a t∗score of 2.0 (with 999 degrees of freedom). 15 The second shortcoming we will discuss is the fact that existing DOP algo- rithms are unable to generalise over the syntactic structures in the data. Corpus- based methods such as the current implementation of DOP, assume that the tree-bank which they employ for acquiring the parser, constitutes a rich enough sample of the domain. It is assumed that the part of the annotation scheme that is actually instantiated in the tree-bank does not under-generate on sentences of the domain. This assumption is not met by our current tree-bank. It turned out that one can expect the tree-bank grammar to generate a parse-space contain- ing the right syntactic/semantic tree only for approximately 90-91% of unseen domain utterances. This ﬁgure constitutes an upper bound on the accuracy for any probabilistic model. Enlarging the tree-bank does not guarantee a good coverage, however. The tree-bank will always represent only a sample of the domain. A solution for this problem is the development of automatic methods for generalising grammars, to enhance their coverage. The goal is to improve both accuracy and coverage by generalising over the structures encountered in the tree-bank. Acknowledgements This research was carried out within the framework of the Priority Programme Language and Speech Technology (TST). The TST-Programme is sponsored by NWO (Dutch Organisation for Scientiﬁc Research). References Alshawi, Hiyan, editor. 1992. The Core Language Engine. ACL-MIT press, Cambridge Mass. van den Berg, M., R. Bod, and R. Scha. 1994. A Corpus-Based Approach to Semantic Interpretation. In Proceedings Ninth Amsterdam Colloquium. ILLC,University of Amsterdam. Bod, Rens. 1993. Using an annotated corpus as a stochastic grammar. In Sixth Conference of the European Chapter of the Association for Computational Linguistics, Utrecht. Bod, Rens and Remko Scha. 1997. Data-oriented language processing: An overview. Technical Report 38, NWO Priority Programme Language and Speech Technology. Bonnema, R. 1996. Data oriented semantics. Master’s thesis, Department of Computational Linguistics, University of Amsterdam. Boros, M., W. Eckert, F. Gallwitz, G. G¨orz, G. Hanrieder, and H. Niemann. 1996. Towards understanding spontaneous speech: Word accuracy vs. con- cept accuracy. In Proceedings of the Fourth International Conference on Spoken Language Processing (ICSLP 96), Philadelphia. 16 Cormen, Leiserson, and Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge Mass. van Noord, Gertjan. 1995. The intersection of ﬁnite state automata and deﬁnite clause grammars. In 33th Annual Meeting of the Association for Computa- tional Linguistics, pages 159–165, MIT Cambridge Mass. cmp-lg/9504026. van Noord, Gertjan. 1997. Evaluation of OVIS2 NLP components. Technical Report 46, NWO Priority Programme Language and Speech Technology. van Noord, Gertjan, Gosse Bouma, Rob Koeling, and Mark-Jan Nederhof. 1996. Conventional natural language processing in the NWO priority programme on language and speech technology. October 1996 Deliverables. Technical Report 28, NWO Priority Programme Language and Speech Technology. van Noord, Gertjan, Gosse Bouma, Rob Koeling, and Mark-Jan"
    },
    {
      "document": "9809113v1.pdf",
      "chunk": "arXiv:cs/9809113v1 [cs.CL] 28 Sep 1998 Improving Tagging Accuracy by Using Voting Taggers Llu´ıs M`arquez, Llu´ıs Padr´o and Horacio Rodr´ıguez Dep. LSI. Technical University of Catalonia c/ Jordi Girona 1-3. 08034 Barcelona {lluism,padro,horacio}@lsi.upc.es Abstract We present a bootstrapping method to de- velop an annotated corpus, which is spe- cially useful for languages with few avail- able resources. The method is being ap- plied to develop a corpus of Spanish of over 5Mw. The method consists on tak- ing advantage of the collaboration of two diﬀerent POS taggers. The cases in which both taggers agree present a higher accu- racy and are used to retrain the taggers. Keywords: POS tagging, Corpus Anno- tation, Bootstrapping techniques 1 Introduction Usual automatic tagging algorithms involve a pro- cess of acquisition (or learning) of a statistical lan- guage model from a previously tagged training cor- pus (supervised learning). The statistical models contain lots of parameters that have to be reliably estimated from the corpus, so the sparseness of the training data is a severe problem. When a new annotated corpus for a language with a reduced amount of available linguistic resources is developed, this issue becomes even more important, since no training corpora are available and the man- ual tagging of a big enough training corpus is very expensive, both in time and human labour. If costly human labour is to be avoided, the accuracy of au- tomatic systems has to be as high as possible, even starting with relatively small manually tagged train- ing sets. In the case of English, existing resources are usu- ally enough, thus existing work on developing cor- pora does not rely much in bootstrapping, although re–estimation procedures are widely used to improve tagger accuracies, specially when limited disam- biguated material is available (Church, 1988; Briscoe et al., 1994; Elworthy, 1994). We ﬁnd automatically tagged corpora which are hand corrected a posteri- ori (Marcus et al., 1993), and fully automatic disam- biguation procedures (Leech et al., 1994; J¨arvinen, 1994) Bootstrapping is one of the methods that can be used to improve the performance of statistical taggers when only small training sets are available. The bootstrapping procedure starts by using a small hand-tagged portion of the corpus as an initial train- ing set. Then, the tagger is used to disambiguate fur- ther material, which is incorporated to the training set and used to retrain the tagger. Since the retrain- ing corpus can be much larger than the initial train- ing corpus we expect to better estimate (or learn) the statistical parameters of the tagging model and to obtain a more accurate tagger. Of course, this procedure can be iterated leading, hopefully, to pro- gressively better language models and more precise taggers. The procedure ends when no more improve- ment is achieved. As stated above, the bootstrapping reﬁning pro- cess is completely automatic. However each step of training corpus enlargement and enrichment could involve a certain amount of manual revision and correction. In this way the process would be semi– automatic. The main problem of this approach is that the retraining material contains errors (because"
    },
    {
      "document": "9905001v1.pdf",
      "chunk": "of the goals of this experiment is to verify whether the result also holds for the WSJ corpus, which is structurally very diﬀerent from ATIS. The WSJ corpus uses 47 POS tags, and its sentences are longer and have more embedded clauses. As in the previous experiment, we construct training sets with annotations of diﬀerent con- stituent types and of diﬀerent numbers of ran- domly chosen labels. Each training set consists of 3600 sentences, and 1780 sentences are used as held-out data. The trained grammars are tested on a set of 2245 sentences. Figure 2 (a) and (b) summarize the outcomes 35 40 45 50 55 60 65 70 75 80 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 Parsing accuracies of directly induced grammars on WSJ test set Number of brackets in the WSJ training data HighP BaseNP BaseP AllNP NotBaseP Rand-100% Rand-75% Rand-50% Rand-25% 35 40 45 50 55 60 65 70 75 80 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 Parsing accuracies of adapted grammars on WSJ test set number of brackets in the WSJ training data HighP BaseNP BaseP AllNP NotBaseP ATIS only Rand-100% Rand-75% Rand-50% Rand-25% (a) (b) Figure 2: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as a function of the number of brackets present in the training corpus. There is a total of 46463 brackets in the training corpus. of this experiment. Many results of this section are similar to the ATIS experiment. Higher- level phrases still provide the most information; the grammars trained on the HighP labels are the only ones that scored as well as the baseline. Labels of simple phrases still seem the least in- formative; scores of grammars trained on BaseP and BaseNP remained far below the baseline. Diﬀerent from the previous experiment, how- ever, the AllNP training sets do not seem to provide as much information for this learning task. This may be due to the increase in the sentence complexity of the WSJ, which further de-emphasized the role of the simple phrases. Thus, grammars trained on AllNP labels have comparable parsing scores to those trained on HighP labels. Also, we do not see as big a gap between the scores of the two induction strate- gies in the HighP case because the adapted grammar’s advantage of having seen annotated ATIS base nouns is reduced. Nonetheless, the adapted grammars still perform 2% better than the directly induced grammars, and this im- provement is statistically signiﬁcant.2 Furthermore, grammars trained on NotBaseP do not fall as far below the baseline and have higher parsing scores than those trained on HighP and AllNP. This suggests that for more complex domains, other linguistic constituents 2A pair-wise t-test comparing the parsing scores of the ten test sets for the two strategies shows 99% conﬁ- dence in the diﬀerence. such as verb phrases3 become more informative. A second goal of this experiment is to test the adaptive strategy under more stringent condi- tions. In the previous experiment, a WSJ-style grammar was retrained for the simpler ATIS corpus."
    },
    {
      "document": "9907013v1.pdf",
      "chunk": "– – ncsubj SBJ s xsubj csubj subj or dobj – – comp – – obj – – dobj (NP after V) o obj2 (2nd NP after V) iobj CLR/DTV i clausal PRD – xcomp e ccomp j Table 1: Rough correspondence between the GR scheme and the functional annotation in the Penn Treebank (ptb) and susanne. author. Inter-annotator agreement was around 95% which is somewhat better than previously reported ﬁgures for syntactic markup (e.g. Leech and Garside, 1991). Marking up was done semi-automatically by ﬁrst generating the set of relations predicted by the evaluation software from the closest system analy- sis to the treebank annotation and then manually correcting and extending these. The mean number of GRs per corpus sentence is 9.72. Table 2 quantiﬁes the distribution of relations occurring in the corpus. The split between modiﬁers and arguments is roughly 60/40, with approximately equal numbers of subjects and complements. Of the latter, 40% are clausal; clausal modiﬁers are almost as prevalent. In strong contrast, clausal subjects are highly infrequent (accounting for only 0.2% of the total). Direct objects are 2.75 times more frequent than indirect objects, which are themselves 7.5 times more prevalent than second objects. The corpus contains sentences belonging to three distinct genres. These are classiﬁed in the original Brown corpus as: A, press reportage; G, belles let- tres; and J, learned writing. Genre has been found to aﬀect the distribution of surface-level syntactic conﬁgurations (Sekine, 1997) and also complement types for individual predicates (Roland & Jurafsky, 1998). However, we observe no statistically signif- Relation # occurrences % occurrences dependent 4690 100.0 mod 2710 57.8 ncmod 2377 50.7 xmod 170 3.6 cmod 163 3.5 arg mod 39 0.8 arg 1941 41.4 subj 993 21.2 ncsubj 984 21.0 xsubj 5 0.1 csubj 4 0.1 subj or dobj 1339 28.6 comp 948 20.2 obj 559 11.9 dobj 396 8.4 obj2 19 0.4 iobj 144 3.1 clausal 389 8.3 xcomp 323 6.9 ccomp 66 1.4 Table 2: Frequency of each type of GR (inclusive of subsumed relations) in the 10K-word corpus. icant diﬀerence in the total numbers of the various grammatical relations across the three genres in the corpus. 3.2 Parser Evaluation We replicated an experiment previously reported by Carroll, Minnen & Briscoe (1998), using a robust lexicalised parser, computing three evaluation mea- sures for each type of relation against the 10K-word test corpus (table 3). The evaluation measures are precision, recall, and F-score (van Rijsbergen, 1979)6 of parser GRs against the test corpus annotation. GRs are in general compared using an equality test, except that we allowed the parser to return mod, subj and clausal relations rather than the more speciﬁc ones they subsume, and to leave unspeci- ﬁed the ﬁller for the type slot in the mod, iobj and clausal relations7. The head and dependent slot ﬁllers are in all cases the base forms of single head words, so for example, ‘multi-component’ heads such as the names of people and companies are reduced to a single word; thus the slot ﬁller corresponding to 6The F-score is a measure combining"
    }
  ]
}
-e 

[POST /search] Query: "BERT vs GPT"
Time: .028342000s
{
  "query": "BERT vs GPT",
  "results": [
    {
      "document": "9906020v1.pdf",
      "chunk": "| . . . PFUNS, CPARTS, GPARTS, CONS, and VARS are disjoint open classes of terminal symbols. A.2 Semantics of TOP Temporal ontology A point structure ⟨PTS, ≺⟩is assumed, where PTS is the set of time-points, and ≺is a binary, transitive, irreﬂexive relation over PTS × PTS. Time is assumed to be discrete, bounded, and linear [12] [28]. tfirst and tlast are the earliest and latest time-points respec- tively. prev(t) and next(t) are used to refer to the immediately previous and following time-points of a t ∈PTS. For S ⊆PTS, minpt(S) and maxpt(S) denote the earliest and latest time-points in S. A period p over ⟨PTS, ≺⟩is a non-empty subset of PTS. Periods are convex, i.e. if t1, t2 ∈p, t3 ∈PTS, and t1 ≺t3 ≺t2, then t3 ∈p. PERIODS is the set of all periods over ⟨PTS, ≺⟩. p1 is a subperiod of p2 (written p1 ⊑p2), iﬀp1, p2 ∈PERIODS and p1 ⊆p2. p1 is a proper subperiod of p2 (written p1 ⊏p2), iﬀp1, p2 ∈PERIODS and p1 ⊂p2. The usual notational conventions apply when specifying the boundaries of periods; e.g. (t1, t2] is an abbreviation for {t ∈PTS | t1 ≺t ⪯t2}. If S is a set of periods, then mxlpers(S) is the set of maximal periods of S. mxlpers(S) ≡ {p ∈S | for no p′ ∈S is it true that p ⊏p′}. TOP model A top model M is an ordered 7-tuple: M = ⟨⟨PTS, ≺⟩, OBJS, fcons, fpfuns, fculms, fgparts, fcparts⟩ where ⟨PTS, ≺⟩is the point structure, PERIODS ⊆OBJS, and fcons, fpfuns, fculms, fgparts, and fcparts are as speciﬁed below: OBJS is a set containing all the objects in the modelled world that can be denoted by top terms, and fcons is a function CONS 7→OBJS. Intuitively, fcons maps each constant to the object it denotes. fpfuns maps each π ∈PFUNS to a function (OBJS)n 7→pow(PERIODS). It is assumed that each predicate symbol π ∈PFUNS is used with a particular arity (number of argu- ments) n. pow(S) denotes the powerset (set of all subsets) of S. For every π ∈PFUNS and every ⟨o1, o2, . . . , on⟩∈(OBJS)n, it must be true that: if p1, p2 ∈fpfuns(π)(o1, o2, . . . , on) and p1 ∪p2 ∈PERIODS, then p1 = p2 Intuitively, fpfuns shows the maximal periods where the situation represented by π(τ1, . . . , τn) holds. fculms is a function that maps each π ∈PFUNS to a function (OBJS)n 7→{T, F}. Intuitively, fculms shows whether or not a situation reaches a climax at the latest time-point where it is ongoing. fgparts is a function that maps each element of GPARTS to a gappy partitioning. A gappy partitioning is a subset S of PERIODS, such that for every p1, p2 ∈S, p1 ∩p2 = ∅, and S p∈S p ̸= PTS. fcparts is a function that maps each element of CPARTS to a complete partitioning. A complete partitioning is a subset S of PERIODS, such that for every p1, p2 ∈S, p1 ∩p2 = ∅, and S p∈S p = PTS. Variable assignment A"
    },
    {
      "document": "9903003v1.pdf",
      "chunk": "Language Resources and Evaluation, 1998. [4] Steven Bird. Computational Phonology: A Constraint-Based Approach. Studies in Natural Language Processing. Cambridge University Press, 1995. [5] Steven Bird. A lexical database tool for quantitative phonological research. In Proceedings of the Third Meeting of the ACL Special Interest Group in Computational Phonology. Association for Computational Linguistics, 1997. [6] Steven Bird and Ewan Klein. Phonological events. Journal of Linguistics, 26:33–56, 1990. [7] Steven Bird and D. Robert Ladd. Presenting autosegmental phonology. Journal of Linguistics, 27:193–210, 1991. [8] Catherine Browman and Louis Goldstein. Articulatory gestures as phonological units. Phonology, 6:201–51, 1989. [9] Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, Robert L. Mercer, and Paul S. Roossin. A statistical approach to machine translation. Computational Linguistics, 16:79–85, 1990. [10] Bob Carpenter. The Logic of Typed Feature Structures, volume 32 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1992. [11] Steve Cassidy and Jonathan Harrington. Emu: An enhanced hierarchical speech data management system. In Proceedings of the Sixth Australian International Conference on Speech Science and Technology, 1996. [www.shlrc.mq.edu.au/emu/]. [12] Giuseppe Di Battista, Peter Eades, Roberto Tamassia, and Ioannis G. Tollis. Algorithms for drawing graphs: an annotated bibliography. [wilma.cs.brown/edu/pub/papers/compgeo/gdbiblio.ps.gz], 1994. [13] Laila Dybkjær, Niels Ole Bernsen, Hans Dybkjær, David McKelvie, and Andreas Mengel. The mate markup framework. MATE Deliverable D1.2, Odense University, 1998. MS-CIS-99-01: Bird & Liberman 47 [14] Konrad Ehlich. HIAT – a transcription system for discourse data. In Jane A. Edwards and Martin D. Lampert, editors, Talking Data: Transcription and Coding in Discourse Research, pages 123–48. Hillsdale, NJ: Erlbaum, 1992. [15] John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathon G. Fiscus, David S. Pallett, and Nancy L. Dahlgren. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM. NIST, 1986. [www.ldc.upenn.edu/lol/docs/TIMIT.html]. [16] Gerald Gazdar and Chris Mellish. Natural Language Processing in Prolog: An Introduction to Computational Linguistics. Addison-Wesley, 1989. [17] J. J. Godfrey, E. C. Holliman, and J. McDaniel. Switchboard: A telephone speech corpus for research and develpment. In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, volume I, pages 517–20, 1992. [18] S. Greenberg. The switchboard transcription project. LVCSR Summer Research Workshop, Johns Hopkins University, 1996. [19] R. Grishman. TIPSTER Architecture Design Document Version 2.3. Technical report, DARPA, 1997. [www.nist.gov/itl/div894/894.02/related projects/tipster/]. [20] Jonathan Harrington, Steve Cassidy, Janet Fletcher, and A. McVeigh. The Mu+ speech database system. Computer Speech and Language, 7:305–31, 1993. [21] Susan R. Hertz. The delta programming language: an integrated approach to nonlinear phonology, phonetics, and speech synthesis. In John Kingston and Mary E. Beckman, editors, Papers in Laboratory Phonology I: Between the Grammar and Physics of Speech, chapter 13, pages 215–57. Cambridge University Press, 1990. [22] Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, and Carol Van Ess-Dykema. Automatic detection of discourse structure for speech recognition and understanding. In Proceedings of the 1997 IEEE Workshop on Speech Recognition and Understanding, pages 88–95, Santa Barbara, 1997. [23] Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca. Switchboard SWBD-DAMSL Labeling Project Coder’s Manual, Draft 13. Technical Report"
    },
    {
      "document": "9906034v1.pdf",
      "chunk": "in Proceedings of the 13th International Conference on Computational Linguistics (COLING-90), Helsinki, Finland, pp. 449–451. Sanﬁlippo, Antonio & Ralf Steinberger: 1997, ‘Automatic selection and ranking of transla- tion candidates’, in Proceedings of the 7th International Conference on Theoretical and Methodological Issues in Machine Translation, Santa Fe, New Mexico, USA, pp. 200–207. Sato, Satoshi & Makoto Nagao: 1990, ‘Toward memory-based translation’, in Proceedings of the 13th International Conference on Computational Linguistics (COLING-90), Helsinki, Finland, vol. 3, pp. 247–252. Sumita, Eiichiro & Hitoshi Iida: 1991, ‘Experiments and prospects of example-based Machine Translation’, in Proceedings of the 29th Annual Meeting of the Association for Computa- tional Linguistics (ACL-91), Berkeley, CA, USA, pp. 185–192. Turcato, Davide, Olivier Laurens, Paul McFetridge & Fred Popowich: 1997, ‘Inﬂectional in- formation in transfer for lexicalist MT’, in Proceedings of the International Conference ‘Recent Advances in Natural Language Processing’ (RANLP-97), Tzigov Chark, Bulgaria, pp. 98–103. Whitelock, Pete: 1994, ‘Shake and bake translation’, in C.J. Rupp, M.A. Rosner & R.L. John- son, eds., Constraints, Language and Computation, London: Academic Press, pp. 339–359. Yarowsky, David: 1995, ‘Unsupervised word sense disambiguation rivaling supervised methods’, in Proceedings of the 33th Annual Meeting of the Association for Computational Linguistics (ACL-95), Cambridge, Massachusetts, USA, pp. 189–196."
    },
    {
      "document": "9812001v3.pdf",
      "chunk": "the 34th Annual Meeting of the Association for Computational Lin- guistics, pages 71–78. [Grefenstette1994] Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston. [Grishman and Sterling1992] Grishman, Ralph and John Sterling. 1992. Acquisition of selectional patterns. Proceedings of the 14th International Conference on Compu- tational Linguistics, pages 658–664. [Grishman and Sterling1994] Grishman, Ralph and John Sterling. 1994. Generalizing automatically generated selectional patterns. Proceedings of the 15th International Conference on Computational Linguistics, pages 742–747. [Grunwald1996] Grunwald, Peter. 1996. A minimum description length approach to grammar inference. In S. Wemter, E. Riloﬀ, and G. Scheler, editors, Symbolic, Connectionist and Statistical Approaches to Learning for Natural Language Pro- cessing, Lecture Note in AI. Springer Verlag, pages 203–216. [Guthrie et al.1991] Guthrie, Joe A., Louise Guthrie, Yorick Wilks, and Homa Aidine- jad. 1991. Subject-dependent co-occurrence and word sense disambiguation. Pro- ceedings of the 29th Annual Meeting of the Association for Computational Linguis- tics, pages 146–152. 116 References [Han and Kobayashi1994] Han, Te Sun and Kingo Kobayashi. 1994. Mathematics of Information and Coding. Iwanami Shoten Publishers, Tokyo. [Haruno and Matsumoto1997] Haruno, Masahiko and Yuji Matsumoto. 1997. Mistake- driven mixture of hierarchical tag context trees. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 230–237. [Haruno, Shirai, and Ooyama1998] Haruno, Masahiko, Satoshi Shirai, and Yoshifumi Ooyama. 1998. Using decision trees to construct a practical parser. Machine Learning (to appear). [Hastings1970] Hastings, W.K. 1970. Monte Carlo sampling method using Markov chains and their applications. Biometrika, 57:97–109. [Helmbold et al.1995] Helmbold, David P., Robert E. Schapire, Yoram Singer, and Man- fred K. Warmuth. 1995. A comparison of new and old algorithm for a mixture estimation problem. Proceedings of the 8th Annual Conference on Computational Learning Theory, pages 69–78. [Hindle1990] Hindle, Donald. 1990. Noun classiﬁcation from predicate-argument struc- tures. Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 268–275. [Hindle and Rooth1991] Hindle, Donald and Mats Rooth. 1991. Structural ambiguity and lexical relations. Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 229–236. [Hindle and Rooth1993] Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120. [Hobbs and Bear1990] Hobbs, Jerry R. and John Bear. 1990. Two principles of parse preference. Proceedings of the 13th International Conference on Computational Linguistics, pages 162–167. [Hogenhout and Matsumoto1996] Hogenhout, Wide R. and Yuji Matsumoto. 1996. Training stochastical grammars on semantical categories. In Stefan Wermter, Ellen Riloﬀ, and Gabriele Scheler, editors, Connectionist, Statistical and Symbolic Ap- proaches to Learning for Natural Language Processing. Springer. [Hogenhout and Matsumoto1997] Hogenhout, Wide R. and Yuji Matsumoto. 1997. A preliminary study of word clustering based on syntactic behavior. CoNLL97:Computational Natural Language Learning, Proceedings of the 1997 Meeting of the ACL Special Interest Group in Natural Language Learning, pages 16–24. References 117 [Inui, Sornlertlamvanich, and Tanaka1998] Inui, Kentaro, Virach Sornlertlamvanich, and Hozumi Tanaka. 1998. Probabilistic GLR parsing: A new formalization and its impact on parsing performance. Journal of Natural Language Processing, 5(3):33– 52. [Iwayama and Tokunaga1995] Iwayama, Makoto and Takenobu Tokunaga. 1995. Cluster-based text categorization: A comparison of category search strategy."
    },
    {
      "document": "9809024v2.pdf",
      "chunk": "length 15 words or less from sections 17 to 23 of the Penn Treebank, parsed using the XTAG English grammar. The results are given in Table F.3. NP Chunking VG Chunking Recall 82.15% 74.51% Precision 83.94% 76.43% Table F.3: Text Chunking performance of the XTAG parser System Training Size Recall Precision Ramshaw & Marcus Baseline 81.9% 78.2% Ramshaw & Marcus 200,000 90.7% 90.5% (without lexical information) Ramshaw & Marcus 200,000 92.3% 91.8% (with lexical information) Supertags Baseline 74.0% 58.4% Supertags 200,000 93.0% 91.8% Supertags 1,000,000 93.8% 92.5% Table F.4: Performance comparison of the transformation based noun chunker and the supertag based noun chunker As described earlier, the results cannot be directly compared with other results in chunking such as in [Ramshaw and Marcus, 1995] since we do not train from the Treebank before testing. However, in earlier work, text chunking was done using a technique called supertagging [Srinivas, 1997b] (which uses the XTAG English grammar) which can be used to train from the Treebank. The comparative results of text chunking between supertagging and other methods of chunking is shown in Figure F.4.2 We also performed experiments to determine the accuracy of the derivation structures pro- duced by XTAG on WSJ text, where the derivation tree produced after parsing XTAG is interpreted as a dependency parse. We took sentences that were 15 words or less from the Penn Treebank [Marcus et al., 1993]. The sentences were collected from sections 17–23 of the Tree- bank. 9891 of these sentences were given at least one parse by the XTAG system. Since XTAG typically produces several derivations for each sentence we simply picked a single derivation 1We treat a sequence of verbs and verbal modiﬁers, including auxiliaries, adverbs, modals as constituting a verb group. 2It is important to note in this comparison that the supertagger uses lexical information on a per word basis only to pick an initial set of supertags for a given word. 280 APPENDIX F. EVALUATION AND RESULTS from the list for this evaluation. Better results might be achieved by ranking the output of the parser using the sort of approach described in [Srinivas et al., 1995]. There were some striking diﬀerences in the dependencies implicit in the Treebank and those given by XTAG derivations. For instance, often a subject NP in the Treebank is linked with the ﬁrst auxiliary verb in the tree, either a modal or a copular verb, whereas in the XTAG derivation, the same NP will be linked to the main verb. Also XTAG produces some dependencies within an NP, while a large number of words in NPs in the Treebank are directly dependent on the verb. To normalize for these facts, we took the output of the NP and VG chunker described above and accepted as correct any dependencies that were completely contained within a single chunk. For example, for the sentence Borrowed shares on the Amex rose to another record, the XTAG and Treebank chunks are shown below. XTAG chunks: [Borrowed shares] [on the Amex] [rose] [to another record] Treebank chunks: [Borrowed shares on the Amex] [rose]"
    }
  ]
}
-e 

