# Homework 3 - Voice-Based AI Assistant (LLM + ASR + TTS)

## ğŸ“š Project Overview

This project implements a local, modular **Voice Agent** powered by:

- ğŸ¤ **ASR**: Converts speech to text (using Faster-Whisper or Whisper.cpp)
- ğŸ§  **LLM**: Responds intelligently using local Ollama (LLaMA3 or similar)
- ğŸ”Š **TTS**: Converts text responses to speech using Edge TTS
- ğŸ›œ **FastAPI**: Exposes the agent as a web API
- ğŸ§  **Memory**: Maintains multi-turn conversation history

---

## ğŸ“ Folder Structure
homework3/
â”œâ”€â”€ audio/ # Stores input .wav and output .mp3 audio files
â”œâ”€â”€ main.py # FastAPI backend: handles API requests
â”œâ”€â”€ llm_client.py # Queries the local Ollama LLM
â”œâ”€â”€ tts.py # Edge-TTS: converts text to speech
â”œâ”€â”€ asr.py # ASR: transcribes audio via Faster-Whisper or Whisper.cpp
â”œâ”€â”€ memory.py # In-memory conversation history manager
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ Dockerfile # Optional: build and run via Docker
â”œâ”€â”€ README.md # Project documentation (this file)

---

## ğŸ› ï¸ Setup & Installation

### 1. Install Python dependencies


pip install -r requirements.txt
### 2. Start your LLM (Ollama)

ollama run llama3
Make sure Ollama is installed and running: https://ollama.com
## ğŸš€ Run the API

uvicorn main:app --reload
Once running, access your API at:
http://localhost:8000/docs

## ğŸ§ Example: Voice Chat API
POST /voice-chat/
Send a .wav file and get a .mp3 voice response from the LLM.
curl -X POST http://localhost:8000/voice-chat/ \
  -F "audio=@audio/user_input.wav" \
  -F "session_id=test123" \
  --output audio/reply.mp3
Response:
A synthesized .mp3 audio file

Auto-saved multi-turn memory for the session

## ğŸ§  Memory Module
memory.py handles per-session conversation history.

Each session has its own list of message turns, useful for multi-turn LLM context.

Want persistent memory? Swap in Redis, SQLite, or LangChain.

## ğŸ”Š ASR Options
You can choose:

Faster-Whisper (default): asr.py loads model via Hugging Face

Whisper.cpp or Vosk: supported with slight config changes

## ğŸ”ˆ TTS: Edge-TTS
Uses Microsoft's edge-tts for fast and natural speech

Generates reply.mp3 in audio/ folder

## ğŸ³ Docker Support (Optional)
Build and run the entire service in a container:

docker build -t voice-agent .
docker run -p 8000:8000 voice-agent
Ollama must run outside Docker on your host machine
Inside container, access Ollama at http://host.docker.internal:11434

## ğŸ§ª Future Work
âœ… Real-time streaming audio input

âœ… WebSocket API for live dialogue

ğŸ”´ Wake-word detection (e.g., with Porcupine or VAD)

ğŸ”´ UI client (React / Streamlit)

ğŸ”´ RAG with vector store + embeddings (personal docs)

## âœï¸ Author
Peter Liu
Assignment for: Machine Learning Engineer in the Generative AI Era
GitHub: @Petercgliu
  
